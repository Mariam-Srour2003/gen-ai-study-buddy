# Study Buddy ğŸ“š

An intelligent AI-powered study companion that transforms your documents into interactive learning experiences. Upload PDFs, ask questions, generate flashcards, and take quizzesâ€”all powered by advanced RAG (Retrieval-Augmented Generation) and LLM technology.

## Features âœ¨

### Core Capabilities

- **ğŸ“„ Document Processing**: Upload and ingest PDF documents with automatic text extraction and chunking
- **ğŸ¤– AI Chat Interface**: Ask questions about your documents and get accurate, cited answers
- **ğŸ“š Flashcard Generation**: Automatically generate custom flashcards from document content
- **ğŸ§  Quiz Generation**: Create multiple-choice quizzes to test your knowledge
- **ğŸ” RAG Pipeline**: Retrieve relevant content from documents to provide context-aware responses
- **ğŸ’¾ Vector Search**: Fast semantic search using FAISS with persistent indexing

### LLM Provider Support

- **Google Gemini**: Full support for Gemini 2.0 Flash and embedding models
- **Ollama**: Local LLM inference with models like Gemma3, Mistral, and more
- **Easy Switching**: Toggle between providers without restarting

### UI Features

- **ğŸ¨ Beautiful Web Interface**: Modern, pastel-themed single-page application
- **ğŸ’¬ Multi-turn Conversations**: Maintain conversation history with multiple chat sessions
- **ğŸ“Œ Citation Tracking**: See which document sections support each answer
- **âš¡ Real-time Responses**: Streaming support for better UX
- **ğŸ“± Responsive Design**: Works on desktop, tablet, and mobile devices

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI        â”‚  â† Vanilla HTML/CSS/JS (SPA)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FastAPI Backend                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Document Ingestion (/rag/ingest)   â”‚
â”‚  â€¢ AI Agent (/agent)                  â”‚
â”‚  â€¢ Health Check (/health)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚       â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ RAG  â”‚  â”‚ LLM Provider   â”‚
    â”‚ Core â”‚  â”‚  - Google      â”‚
    â””â”€â”€â”€â”€â”¬â”€â”˜  â”‚  - Ollama      â”‚
         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector Store      â”‚
    â”‚ (FAISS Indices)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### Prerequisites

- **Python**: 3.12 or higher
- **Git**: For cloning the repository
- **pip/uv**: Python package manager (uv recommended for speed)

### Step 1: Clone Repository

```bash
git clone https://github.com/Mariam-Srour2003/gen-ai-study-buddy.git
cd gen-ai-study-buddy
```

### Step 2: Set Up Python Environment

Using **uv** (recommended, faster):
```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv sync
```

Using traditional **pip**:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .
```

### Step 3: Configure Environment Variables

Create a `.env` file in the project root:

#### For Google Gemini Provider

```bash
GOOGLE_API_KEY=your_google_api_key_here
LLM_PROVIDER=google
EMBEDDING_PROVIDER=google
```

Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey).

#### For Ollama Provider (Local)

```bash
LLM_PROVIDER=ollama
EMBEDDING_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_LLM_MODEL=gemma3:4b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest
```

Install Ollama from [ollama.ai](https://ollama.ai) and pull models:
```bash
ollama pull gemma3:4b
ollama pull nomic-embed-text
ollama serve  # In another terminal
```

### Step 4: Run the Application

```bash
# With Ollama (ensure ollama serve is running in another terminal)
export LLM_PROVIDER=ollama
export EMBEDDING_PROVIDER=ollama
uvicorn src.study_buddy.main:app --reload --host 0.0.0.0 --port 8000
```

```bash
# With Google
export GOOGLE_API_KEY=your_key_here
export LLM_PROVIDER=google
export EMBEDDING_PROVIDER=google
uvicorn src.study_buddy.main:app --reload --host 0.0.0.0 --port 8000
```

Or use uv:
```bash
uv run uvicorn src.study_buddy.main:app --reload --host 0.0.0.0 --port 8000
```

### Step 5: Access the Application

Open your browser and navigate to:
```
http://localhost:8000
```

## Usage Guide

### 1. Upload a Document

- Click the **Upload** button (ğŸ“¤ icon) in the bottom left
- Select a PDF file from your computer
- Wait for processing to complete

### 2. Chat with Your Document

- Type questions in the chat box
- Questions are answered using context from your uploaded document
- Each answer includes citations showing which parts of the document were used

### 3. Generate Flashcards

- Click the **ğŸ“š Flashcards** button in the top right
- Select the number of cards (4, 5, 10, or 15)
- Click **Generate** to create flashcards
- Click cards to flip them and reveal answers

### 4. Create Quizzes

- Click the **ğŸ§  Quiz** button in the top right
- Select the number of questions (3, 5, or 10)
- Click **Generate** to create a quiz
- Answer questions and get immediate feedback

### 5. Switch Providers

- Use the **Provider** dropdown in the header
- Select between **Google** or **Ollama**
- Click **Apply** to switch

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_PROVIDER` | google | LLM provider: `google` or `ollama` |
| `EMBEDDING_PROVIDER` | google | Embedding provider: `google` or `ollama` |
| `GOOGLE_API_KEY` | - | Required for Google provider |
| `OLLAMA_BASE_URL` | http://localhost:11434 | Ollama server URL |
| `OLLAMA_LLM_MODEL` | gemma3:4b | Ollama LLM model |
| `OLLAMA_EMBEDDING_MODEL` | nomic-embed-text:latest | Ollama embedding model |
| `CHUNK_SIZE` | 512 | Document chunk size |
| `CHUNK_OVERLAP` | 50 | Overlap between chunks |
| `TOP_K_RESULTS` | 3 | Number of retrieved results for RAG |
| `BASE_STORAGE_PATH` | ./storage_data | Storage location for indices and metadata |

### Storage Structure

```
storage_data/
â”œâ”€â”€ faiss_indices/          # Vector indices (.index and .pkl files)
â”œâ”€â”€ metadata/               # Chunk metadata and embeddings
â””â”€â”€ uploaded_files/         # Original PDF files
```

## Project Structure

```
gen-ai-study-buddy/
â”œâ”€â”€ src/study_buddy/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”œâ”€â”€ models.py               # Pydantic models (Flashcard, MCQQuestion, etc.)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py         # Configuration and environment variables
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routers.py          # RAG and health endpoints
â”‚   â”‚   â”œâ”€â”€ agent_router.py     # Agent endpoints
â”‚   â”‚   â””â”€â”€ dependencies.py     # Dependency injection
â”‚   â”œâ”€â”€ rag_qa/
â”‚   â”‚   â”œâ”€â”€ qa.py               # RAG pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ vectorstore.py      # FAISS vector store
â”‚   â”‚   â””â”€â”€ embedder.py         # Embedding providers
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py       # PDF text extraction
â”‚   â”‚   â””â”€â”€ chunker.py          # Document chunking
â”‚   â”œâ”€â”€ flashcards/
â”‚   â”‚   â”œâ”€â”€ generator.py        # Flashcard generation logic
â”‚   â”‚   â””â”€â”€ templates.py        # Prompt templates
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ study_agent.py      # LangChain agent
â”‚   â”‚   â””â”€â”€ tools.py            # Agent tools
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ metadata.py         # Metadata persistence
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ app_ui.py           # Web UI
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_basic.py           # Basic tests
â”œâ”€â”€ pyproject.toml              # Project configuration
â””â”€â”€ .env                        # Environment variables
```

## API Endpoints

### Core Endpoints

#### Upload PDF
```
POST /rag/ingest/pdf
Content-Type: multipart/form-data

Body:
  file: <PDF file>

Response:
{
  "doc_id": "unique_document_id",
  "status": "success"
}
```

#### Ask Question / Generate Content
```
POST /agent
Content-Type: application/json

Body:
{
  "mode": "explain|summarize|flashcards|mcq",
  "input": "Your question",
  "doc_id": "document_id",
  "num_questions": 5  # For flashcards/mcq
}

Response:
{
  "message": "...",
  "sources": [...]  # Citations
}
```

#### Health Check
```
GET /health
```

#### Provider Management
```
GET /providers
POST /providers/switch
```

## Troubleshooting

### Issue: "GOOGLE_API_KEY is required"

**Solution**: Set your Google API key in `.env`:
```bash
export GOOGLE_API_KEY=your_key_here
```

### Issue: Ollama connection error

**Solution**: Ensure Ollama is running:
```bash
ollama serve
```

And verify the connection:
```bash
curl http://localhost:11434/api/tags
```


## Performance Tips

- **Chunk Size**: Larger chunks (1024) for long-form content, smaller (256) for precise answers
- **Top K Results**: Increase to 5-10 for broader context, decrease to 2-3 for precision
- **Model Selection**: 
  - **Fast**: Phi 2.7B, Gemma 7B
  - **Balanced**: Mistral 7B, Gemma3 4B
  - **Best Quality**: Mistral 12B, Gemini

## Development

### Running Tests

```bash
python -m pytest tests/
```

## Technologies Used

- **FastAPI**: Modern Python web framework
- **LangChain**: LLM orchestration and RAG
- **FAISS**: Vector similarity search
- **Pydantic**: Data validation
- **Uvicorn**: ASGI server
- **PDFPlumber**: PDF text extraction
- **Google Generative AI**: LLM and embedding APIs
- **Ollama**: Local LLM inference

**Happy Studying! ğŸ“šâœ¨**
